{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From pandas to production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Let's train a model for twitter sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "Dataset source: https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -f ./trainingandtestdata.zip ]; then\n",
    "    wget -q http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
    "    unzip -n trainingandtestdata.zip\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set the columns\n",
    "columns = ['polarity', 'tweetid', 'date', 'query_name', 'user', 'text']\n",
    "dftrain = pd.read_csv('training.1600000.processed.noemoticon.csv',\n",
    "                      header = None,\n",
    "                      encoding ='ISO-8859-1')\n",
    "dftest = pd.read_csv('testdata.manual.2009.06.14.csv',\n",
    "                     header = None,\n",
    "                     encoding ='ISO-8859-1')\n",
    "dftrain.columns = columns\n",
    "dftest.columns = columns\n",
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Reference: https://docs.bentoml.org/en/latest/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentiment_lr = Pipeline([\n",
    "                         ('count_vect', CountVectorizer(min_df = 100,\n",
    "                                                        stop_words = 'english')), \n",
    "                         ('lr', LogisticRegression())])\n",
    "sentiment_lr.fit(dftrain.text, dftrain.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xtest, ytest = dftest.text[dftest.polarity!=2], dftest.polarity[dftest.polarity!=2]\n",
    "print(classification_report(ytest,sentiment_lr.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Package the model into an API service\n",
    "\n",
    "Now that we have a working machine learning model built with pandas, our goal is to share this model with our customers so they can make predictions on their data. \n",
    "\n",
    "Our customers do not have access to the jupyter notebook with our trained model, so we need to do some clever packaging to serve this model to our customers\n",
    "\n",
    "A popular way to deploy our model to our customers is through a REST API, so that our customers can make a web request with their data, and get the model predictions as in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Model Service using BentoML\n",
    "BentoML is a python framework that makes it super easy to move your ML models as web APIs, so that your users can send data to a web URL and get the predictions of your model as the response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile sentiment_analysis_service.py\n",
    "\n",
    "import bentoml\n",
    "\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "from bentoml.service.artifacts.common import PickleArtifact\n",
    "from bentoml.adapters import JsonInput\n",
    "\n",
    "@bentoml.artifacts([PickleArtifact('model')])  # picke your trained model so that it can run on the server\n",
    "@bentoml.env(pip_packages=[\"scikit-learn\", \"pandas\"])  # specify the packages that your model depends on\n",
    "class SKSentimentAnalysis(bentoml.BentoService):\n",
    "\n",
    "    sentiment_names = {\n",
    "        0: \"very negative\",\n",
    "        1: \"somewhat negative\",\n",
    "        2: \"neutral\",\n",
    "        3: \"somewhat positive\",\n",
    "        4: \"very positive\",\n",
    "    }\n",
    "    \n",
    "    @bentoml.api(input=JsonInput())\n",
    "    def predict(self, parsed_json):\n",
    "        \"\"\"\n",
    "        Sentiment prediction API service\n",
    "        \n",
    "        Expected input format:\n",
    "        [\"Some text to predict the sentiment...\", \"some more text to predict sentiment\"]\n",
    "\n",
    "        Output format:\n",
    "        {\"sentiment_score\": 4, \"sentiment\": \"Very Positive\", \"tweet\": \"Tweet text to predict the sentiment...\"}\n",
    "        \"\"\"\n",
    "        texts = parsed_json\n",
    "        predictions = self.artifacts.model.predict(texts)\n",
    "        res = []\n",
    "        for idx, pred in enumerate(predictions):\n",
    "            res.append({\n",
    "                \"sentiment_score\": pred, \n",
    "                \"sentiment\": self.sentiment_names[pred], \n",
    "                \"text\": texts[idx]\n",
    "            })\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Package the model service and the trained model into a Docker file for easy deployment\n",
    "\n",
    "This step packages the model service with the trained model from step 1. After we package the API service with the model, we are ready to start serving some requests.\n",
    "\n",
    "One common way of distributing this model API server for production deployment, is via Docker containers. And BentoML provides a convenient way to do that.\n",
    "\n",
    "### 1) import the custom BentoService defined above\n",
    "from sentiment_analysis_service import SKSentimentAnalysis\n",
    "\n",
    "### 2) `pack` it with required artifacts, i.e. the trained model from step 1\n",
    "bento_service = SKSentimentAnalysis()\n",
    "bento_service.pack('model', sentiment_lr)\n",
    "\n",
    "### 3) save your BentoSerivce to file archive\n",
    "saved_path = bento_service.save()\n",
    "!bentoml serve SKSentimentAnalysis:latest --port=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_and_serve():\n",
    "    # 1) import the custom BentoService defined above\n",
    "    from sentiment_analysis_service import SKSentimentAnalysis\n",
    "\n",
    "    # 2) `pack` it with required artifacts, i.e. the trained model from step 1\n",
    "    bento_service = SKSentimentAnalysis()\n",
    "    bento_service.pack('model', sentiment_lr)\n",
    "\n",
    "    # 3) save your BentoSerivce to file archive\n",
    "    saved_path = bento_service.save()\n",
    "\n",
    "    # 4) Start a REST API model server with the BentoService saved above to serve the model\n",
    "    !bentoml serve SKSentimentAnalysis:latest --port=5000\n",
    "        \n",
    "package_and_serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Error monitoring with Sentry\n",
    "\n",
    "That's great, we have a Model API so we are done now, right? Not quite.\n",
    "\n",
    "There are a few more steps before we can call our model API ready for production. For example, what happens if someone does not send the \"tweet\" field in the request?\n",
    "\n",
    "Well, our API service expects that the user sends a \"tweet\" key in the JSON, so if they fail to send one, our server will error out.\n",
    "\n",
    "As the author of the API service, you might want to know when the service encounters unexpected errors such as this\n",
    "\n",
    "Sentry is great for this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by installing sentry\n",
    "!pip3 install sentry-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sentiment_analysis_service.py\n",
    "# Now let's modify our service to use sentry\n",
    "\n",
    "import bentoml\n",
    "\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "from bentoml.service.artifacts.common import PickleArtifact\n",
    "from bentoml.adapters import JsonInput\n",
    "\n",
    "# Edit 1: Import sentry\n",
    "import sentry_sdk\n",
    "import logging\n",
    "from sentry_sdk.integrations.logging import LoggingIntegration\n",
    "\n",
    "# All of this is already happening by default!\n",
    "sentry_logging = LoggingIntegration(\n",
    "    level=logging.INFO,        # Capture info and above as breadcrumbs\n",
    "    event_level=logging.ERROR  # Send errors as events\n",
    ")\n",
    "sentry_sdk.init(\n",
    "    dsn=\"https://651a85506d1d4be3876a224d8a92eb2c@o1140265.ingest.sentry.io/6197278\",\n",
    "    integrations=[sentry_logging]\n",
    ")\n",
    "\n",
    "\n",
    "@bentoml.artifacts([PickleArtifact('model')])\n",
    "@bentoml.env(infer_pip_packages=True)\n",
    "class SKSentimentAnalysis(bentoml.BentoService):\n",
    "\n",
    "    sentiment_names = {\n",
    "        0: \"very negative\",\n",
    "        1: \"somewhat negative\",\n",
    "        2: \"neutral\",\n",
    "        3: \"somewhat positive\",\n",
    "        4: \"very positive\",\n",
    "    }\n",
    "    \n",
    "    @bentoml.api(input=JsonInput())\n",
    "    @bentoml.env(pip_packages=[\"scikit-learn\", \"pandas\", \"sentry-sdk==1.5.4\"])\n",
    "    def predict(self, parsed_json):\n",
    "        \"\"\"\n",
    "        Sentiment prediction API service\n",
    "        \n",
    "        Expected input format:\n",
    "        {\"tweet\": \"Tweet text to predict the sentiment...\"}\n",
    "\n",
    "        Output format:\n",
    "        {\"sentiment_score\": 4, \"sentiment\": \"Very Positive\", \"tweet\": \"Tweet text to predict the sentiment...\"}\n",
    "        \"\"\"\n",
    "        # Edit 3: Update the code to capture exceptions to sentry\n",
    "        try:\n",
    "            texts = parsed_json\n",
    "            predictions = self.artifacts.model.predict(texts)\n",
    "            res = []\n",
    "            for idx, pred in enumerate(predictions):\n",
    "                # Edit 4: make a deliberate mistake\n",
    "                1/ 0 # raise a ZeroDivisionError\n",
    "                res.append({\n",
    "                    \"sentiment_score\": pred, \n",
    "                    \"sentiment\": self.sentiment_names[pred], \n",
    "                    \"text\": texts[idx]\n",
    "                })\n",
    "\n",
    "            return res        \n",
    "        except:\n",
    "            sentry_sdk.capture_exception()\n",
    "            return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_and_serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Measure latency and throughput with Prometheus\n",
    "Now that the API service is deployed, I would want to start measuring myAPI performance next. Latency and Throughput are two important API metrics that are useful to measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sentiment_analysis_service.py\n",
    "# Now let's modify our service to use prometheus to measure latency\n",
    "\n",
    "import bentoml\n",
    "\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "from bentoml.service.artifacts.common import PickleArtifact\n",
    "from bentoml.adapters import JsonInput\n",
    "\n",
    "import sentry_sdk\n",
    "import logging\n",
    "from sentry_sdk.integrations.logging import LoggingIntegration\n",
    "\n",
    "# All of this is already happening by default!\n",
    "sentry_logging = LoggingIntegration(\n",
    "    level=logging.INFO,        # Capture info and above as breadcrumbs\n",
    "    event_level=logging.ERROR  # Send errors as events\n",
    ")\n",
    "sentry_sdk.init(\n",
    "    dsn=\"https://your_dsn_goes_here@o1140265.ingest.sentry.io/934290\",\n",
    "    integrations=[sentry_logging]\n",
    ")\n",
    "\n",
    "\n",
    "# Edit 1: Import prometheus\n",
    "from prometheus_client import Summary\n",
    "REQUEST_TIME = Summary('request_processing_time', 'Time spend processing request')\n",
    "\n",
    "\n",
    "@bentoml.artifacts([PickleArtifact('model')])\n",
    "@bentoml.env(infer_pip_packages=True)\n",
    "class SKSentimentAnalysis(bentoml.BentoService):\n",
    "\n",
    "    sentiment_names = {\n",
    "        0: \"very negative\",\n",
    "        1: \"somewhat negative\",\n",
    "        2: \"neutral\",\n",
    "        3: \"somewhat positive\",\n",
    "        4: \"very positive\",\n",
    "    }\n",
    "    \n",
    "    # Edit 2: Monitor request time on the API\n",
    "    @REQUEST_TIME.time()\n",
    "    @bentoml.api(input=JsonInput())\n",
    "    def predict(self, parsed_json):\n",
    "        \"\"\"\n",
    "        Sentiment prediction API service\n",
    "        \n",
    "        Expected input format:\n",
    "        {\"tweet\": \"Tweet text to predict the sentiment...\"}\n",
    "\n",
    "        Output format:\n",
    "        {\"sentiment_score\": 4, \"sentiment\": \"Very Positive\", \"tweet\": \"Tweet text to predict the sentiment...\"}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            texts = parsed_json\n",
    "            if len(texts) == 12:\n",
    "                import time\n",
    "                time.sleep(5)\n",
    "            predictions = self.artifacts.model.predict(texts)\n",
    "            res = []\n",
    "            for idx, pred in enumerate(predictions):\n",
    "                res.append({\n",
    "                    \"sentiment_score\": pred, \n",
    "                    \"sentiment\": self.sentiment_names[pred], \n",
    "                    \"text\": texts[idx]\n",
    "                })\n",
    "\n",
    "            return res        \n",
    "        except:\n",
    "            sentry_sdk.capture_exception()\n",
    "            return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_and_serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send prediction request to REST API server\n",
    "\n",
    "Run the following command in terminal to make a HTTP request to the API server:\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[\"some new text, sweet noodles\", \"happy time\", \"sad day\"]' \\\n",
    "localhost:5000/predict\n",
    "```\n",
    "\n",
    "You can also view all availabl API endpoints at [localhost:5000](localhost:5000), or look at prometheus metrics at [localhost:5000/metrics](localhost:5000/metrics) in browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sentiment_analysis_service.py\n",
    "# Now let's modify our service to use prometheus to measure custom metrics\n",
    "\n",
    "import bentoml\n",
    "\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "from bentoml.service.artifacts.common import PickleArtifact\n",
    "from bentoml.adapters import JsonInput\n",
    "\n",
    "import sentry_sdk\n",
    "import logging\n",
    "from sentry_sdk.integrations.logging import LoggingIntegration\n",
    "\n",
    "# All of this is already happening by default!\n",
    "sentry_logging = LoggingIntegration(\n",
    "    level=logging.INFO,        # Capture info and above as breadcrumbs\n",
    "    event_level=logging.ERROR  # Send errors as events\n",
    ")\n",
    "sentry_sdk.init(\n",
    "    dsn=\"https://651a85506d1d4be3876a224d8a92eb2c@o1140265.ingest.sentry.io/6197278\",\n",
    "    integrations=[sentry_logging]\n",
    ")\n",
    "\n",
    "from prometheus_client import Summary\n",
    "REQUEST_TIME = Summary('request_processing_time', 'Time spend processing request')\n",
    "# Edit 1: Create a custom metric\n",
    "REQUEST_TEXT_LEN = Summary('request_text_len', 'Length of texts array for inference')\n",
    "\n",
    "\n",
    "@bentoml.artifacts([PickleArtifact('model')])\n",
    "@bentoml.env(infer_pip_packages=True)\n",
    "class SKSentimentAnalysis(bentoml.BentoService):\n",
    "\n",
    "    sentiment_names = {\n",
    "        0: \"very negative\",\n",
    "        1: \"somewhat negative\",\n",
    "        2: \"neutral\",\n",
    "        3: \"somewhat positive\",\n",
    "        4: \"very positive\",\n",
    "    }\n",
    "    \n",
    "    @REQUEST_TIME.time()\n",
    "    @bentoml.api(input=JsonInput())\n",
    "    def predict(self, parsed_json):\n",
    "        \"\"\"\n",
    "        Sentiment prediction API service\n",
    "        \n",
    "        Expected input format:\n",
    "        {\"tweet\": \"Tweet text to predict the sentiment...\"}\n",
    "\n",
    "        Output format:\n",
    "        {\"sentiment_score\": 4, \"sentiment\": \"Very Positive\", \"tweet\": \"Tweet text to predict the sentiment...\"}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            texts = parsed_json\n",
    "            predictions = self.artifacts.model.predict(texts)\n",
    "            res = []\n",
    "            # Edit 2: Monitor request text lengt on the API\n",
    "            REQUEST_TEXT_LEN.observe(len(texts))\n",
    "            for idx, pred in enumerate(predictions):\n",
    "                res.append({\n",
    "                    \"sentiment_score\": pred, \n",
    "                    \"sentiment\": self.sentiment_names[pred], \n",
    "                    \"text\": texts[idx]\n",
    "                })\n",
    "\n",
    "            return res        \n",
    "        except:\n",
    "            sentry_sdk.capture_exception()\n",
    "            return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repackage and serve\n",
    "# 1) import the custom BentoService defined above\n",
    "from sentiment_analysis_service import SKSentimentAnalysis\n",
    "\n",
    "# 2) `pack` it with required artifacts, i.e. the trained model from step 1\n",
    "bento_service = SKSentimentAnalysis()\n",
    "bento_service.pack('model', sentiment_lr)\n",
    "\n",
    "# 3) save your BentoSerivce to file archive\n",
    "saved_path = bento_service.save()\n",
    "!bentoml serve SKSentimentAnalysis:latest --port=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
